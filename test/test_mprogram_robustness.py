"""
Test for evaluating the robustness of a Mellea program using probes
generated by the BenchDrift pipeline.
"""
import sys
import os
from typing import Any

# Ensure the new tool and Mellea can be imported
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

import yaml
from pathlib import Path

from mellea import start_session
from mellea.backends.types import ModelOption
from mellea_contribs.tools.benchdrift_runner import (
    run_benchdrift_pipeline,
    analyze_robustness_from_probes
)

def test_m_program_robustness():
    """
    An end-to-end test that:
    1. Starts a Mellea session
    2. Defines an m-program
    3. Generates probes using the BenchDrift pipeline with m-program
    4. Analyzes robustness from results
    5. Asserts the agent performs consistently
    """
    # --- 1. Define the baseline problem (catering order) ---
    context = ""
    baseline_question = """RULES:
You are calculating total cost for a catering order.
Base price is $15 per person.
Groups of 20 or more get a 10% discount.
Weekend events have a $50 surcharge.
Delivery within 10 miles is free, beyond that costs $2 per mile.

EXAMPLES:
- 15 people, weekday, 5 miles: 15 × $15 = $225
- 25 people, weekend, 8 miles: (25 × $15 × 0.9) + $50 = $387.50
- 30 people, weekday, 15 miles: (30 × $15 × 0.9) + (5 × $2) = $415

QUESTION:
A company is ordering catering for 22 people for a Saturday event. The venue is 12 miles away. What is the total cost?"""
    ground_truth_answer = "$351"

    # --- 2. Start Mellea session with Ollama ---
    try:
        m = start_session(
            backend_name="ollama",
            model_id="granite3.3:8b",
            model_options={ModelOption.TEMPERATURE: 0.1}
        )
    except Exception as e:
        print(f"Failed to start Mellea session: {e}")
        print("Skipping test. Ensure ollama is running: ollama serve")
        print("And model is available: ollama pull granite3.3:8b")
        return

    # --- 3. Define m-program ---
    # Now m_program receives only the QUESTION (which may be a variant from BenchDrift)
    # and combines it with the stable CONTEXT via grounding_context
    call_count = [0]  # Use list to track calls in closure

    def m_program(question: str) -> Any:
        """
        M-program: Mellea-wrapped agent that answers via m.instruct
        """
        call_count[0] += 1

        # Simple instruct call without grounding_context for now
        response = m.instruct(question)
        answer = response.value if hasattr(response, 'value') else response

        # # DEBUG: Uncomment to print m-program responses
        # import sys
        # msg = f"\n{'='*70}\n"
        # msg += f"[M-PROGRAM CALL #{call_count[0]}]\n"
        # msg += f"Question: {question}\n"
        # msg += f"Response: {str(answer)}\n"
        # msg += f"{'='*70}\n"
        # print(msg, file=sys.stderr, flush=True)

        return answer

    # --- 4. Load BenchDrift Configuration ---
    # Load config from YAML file
    config_path = Path(__file__).parent.parent / 'config' / 'benchdrift_config.yaml'
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)

    # Extract parameter values (ignore comments)
    benchdrift_config = {}
    for key, value in config.items():
        if isinstance(value, (str, int, float, bool)):
            benchdrift_config[key] = value

    # Modify config if needed (example - commented out)
    # benchdrift_config['max_workers'] = 8
    # benchdrift_config['use_persona'] = True

    # --- 5. Generate Probes with BenchDrift + M-Program ---
    # This calls the full BenchDrift pipeline (3 stages) with m-program
    # Note: RITS_API_KEY environment variable must be set.
    # Now passing only the QUESTION - BenchDrift will generate variants of just the question
    try:
        probes = run_benchdrift_pipeline(
            baseline_problem=baseline_question,
            ground_truth_answer=ground_truth_answer,
            m_program_callable=m_program,
            mellea_session=m,
            config_overrides=benchdrift_config
        )
    except Exception as e:
        print(f"BenchDrift pipeline failed: {e}")
        import traceback
        traceback.print_exc()
        print("Skipping robustness test. Ensure RITS_API_KEY is set and BenchDrift dependencies are met.")
        return

    assert probes is not None
    assert len(probes) > 1

    # --- 6. Analyze Robustness from Probes ---
    robustness = analyze_robustness_from_probes(probes)

    # --- 7. Assert the Results ---
    print("\n--- Robustness Testing Results ---")
    print(f"Overall pass rate: {robustness['overall_pass_rate']:.2%}")
    print(f"Total variants tested: {robustness['total_variants']}")
    print(f"Passed: {robustness['pass_count']}, Failed: {robustness['fail_count']}")
    print(f"Drift analysis: {robustness['drift_analysis']}")

    # Check that the test ran successfully (don't assert on pass rate yet)
    assert "error" not in robustness
    # TODO: Improve m-program or model to achieve higher pass rates
    # For now, just verify the pipeline runs end-to-end
    print(f"\n✅ Integration test completed successfully!")
    print(f"   Current pass rate: {robustness['overall_pass_rate']:.2%}")
    print(f"   (This may be low due to model capability on math problems)")


if __name__ == "__main__":
    test_m_program_robustness()
